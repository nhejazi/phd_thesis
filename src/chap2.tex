\chapter{Methodology}

\section{Data and Statistical Model}\label{data}

Others have proposed using estimators developed for low-dimensional causal
inference problems to derive nonparametric estimators of association in the
context of high-dimensional biomarker discovery studies
\cite{tuglus2011targeted}. In such cases, the goals of analyses are similar to
those of more typical parametric approaches, but the approach is based on
nonparametric estimands and can be estimated with data-adaptive techniques. Such
data structures typically consist of large matrices of biological expression
values as well as tables of phenotypic information on each subject. In
particular, in later sections, we will illustrate the use of our technique on
data generated by the \textit{Illumina Human Ref-8 BeadChips} platform, from a
study which included expression measures on about $22,000$ genes as well as
phenotypic information, on a sample of $125$ subjects. The analysis aims to
evaluate the association of an environmental exposure (benzene) on the
expression measures of the roughly $22,000$ genes simultaneously, controlling
for the several aforementioned confounders. In our analysis, we consider three
potential confounding factors on the relationship of exposure and expression:
age, sex, and smoking status. This problem setup is easily generalizable to
situations with greater numbers of potential biomarkers and confounders.
Ultimately, the aim of analyzing such data sets is to rank the importance of a
set of candidate biomarkers based on their independent associations with a
treatment variable. In order to build a ranking of biomarkers, we start by
defining a variable importance measure (VIM)~\cite{van2011targeted}.

Let $O = (W, A, Y)$ represent a random variable defined on the observed data,
where $W$ are the confounders, $A$ the exposure of interest, and
$Y = (Y_b, b = 1, \dots, B)$ a vector of potential biomarkers. Note that we
observe $n$ i.i.d.~copies of the random variable $O$, such that
$O_i = \{O_1, \dots, O_n\}$. Further, let $O \sim P_0 \in \mathscr{M}$, where
$P_{0}$ is the unknown probability distribution of the full data, contained in
an infinite-dimensional statistical model $\mathscr{M}$. For the specific data
set described in above, $W = (W_{1}, W_{2}, W_{3}, W_{4}, W_{5})$, where age
($W_{1}$) is a continuous measure, gender ($W_{2}$) is binary, smoking status
($W_{3}$) is binary, BMI ($W_{4}$) is a continuous measure, and alcohol
consumption ($W_{5}$) is binary; $A$ is a binary exposure; and $Y_{b}$ are miRNA
expression measures.

\section{The Target Parameter}\label{targetparam}

To define the parameter of interest, generally, let $\Psi(P_{0})$ be the
target parameter based on a function $\Psi$ that maps the probability
distribution $P_{0}$ into the target feature of interest. Thus, the parameter
$\Psi(P_{0})$ is a function of the unknown probability distribution $P_{0}$,
defined on the (unobserved) full data. Let $P_{n}$ represent the empirical
distribution of the observed data $O_{1},O_{2}, \dots, O_{n}$. Though we focus
on cases where the $O_i$ are i.i.d., the following is easily generalizable when
the data are clustered (e.g., as repeated samples from the same biological
unit). We are interested in substitution estimators of the form
$\Psi(P_{n}^{*})$ --- that is, we apply the same mapping ($\Psi$) but to the
empirical distribution $P_n$ to derive our estimate (e.g., $\Psi$ could merely
be the expectation operator). In using this general definition, we expand the
parameters of interest beyond coefficients in a misspecified parametric
statistical model, by defining a parameter as a feature of the true probability
distribution $P_{0}$ of the full data. Specifically, we propose here what is
referred to as a targeted variable importance
measure~\cite{bembom2009biomarker}:

\begin{eqnarray}
\label{eqn:targetparam}
\Psi_{b} \equiv \Psi_{b}(P_{0}) = \E _{W,0}[\E_{0}(Y_{b} \mid A = 1, W) -
  \E_{0}(Y_{b} \mid A = 0, W)].
\end{eqnarray}

The parameter delineated in (~\ref{eqn:targetparam}) above is generally referred
to as the average treatment effect, often denoted simply as the
ATE~\cite{rosenbaum1983central}. When the assumptions underlying the causal
model, through which the target parameter is defined, do not hold, the estimand
of the parameter of interest takes on a statistical interpretation:
specifically, the difference of means within the strata $W$, averaged across
levels of the treatment $A$. It has been shown that, under identifiability
assumptions (e.g., no unmeasured confounding), this parameter can be
statistically estimated via targeted maximum likelihood
estimation~\cite{van2011targeted}. Such parameters are significant in that they
are not defined explicitly via parametric statistical models, leaving one free
to fit the requisite models data-adaptively, minimizing assumptions wherever
possible, and yet still estimating a relatively simple parameter with rich
scientific interpretation.

\section{Statistical Estimation}\label{estimation}

As noted previously in Section~\ref{targetparam}, the target parameter is
defined as a feature of the unknown probability distribution $P_{0}$. While
there are several general classes of estimators available for estimating
$\Psi$, here we focus on a substitution estimator as noted above.
Examining (~\ref{eqn:targetparam}), one can anticipate that a substitution
estimator will rely on estimates of two components of the data-generating
mechanism, $P_0$: $\E_0 (Y \mid A = a, W)$ and $P_0(W)$, or the true regression
of $Y$ on $(A, W)$ and the marginal distribution of $W$. Let
$Q^{b}_{0}(A, W) \equiv \E_{0}(Y_{b} \mid A, W)$, and $Q^{b}_{n}(A, W)$ an
estimate of this regression. If we use the empirical distribution to estimate
the joint marginal distribution of the $W$, then a substitution estimator is:

\begin{equation}
\label{subest}
  \Psi_b(P_{n}^{*}) = \frac{1}{n}\sum_{i = 1}^{n} Q^{b}_{n}(A_i = 1, W_{i}) -
  Q^{b}_{n}(A_i = 0, W_{i}).
\end{equation}

Below, we discuss recommendations for an initial estimate of $Q_0$, using the
Super Learner algorithm, and a bias-reducing augmentation (targeted minimum
loss-based estimation) with optimal properties for minimizing the error of
estimation and deriving robust inference.

\subsection{Using the Super Learner algorithm}\label{superlearner}

The first step in the two-stage TMLE procedure is to derive an initial estimate
of $Q^{b}_{0}$, referred to as $Q^{(b, 0)}_{n}$. For instance, one may assume
a parametric statistical model that results in (\ref{targetparam}) being
equivalent to a regression coefficient (e.g., $Q^{b}_{0}(A, W) = \alpha^{b} +
\beta^{b}_{A}A + \beta^{b}_{W}W$). By defining (~\ref{targetparam}) in a
nonparametric statistical model, using data-adaptive tools to estimate
$Q^{b}_{0}$, we avoid settings wherein estimators based on parametric models
would be inconsistent. Specifically, given that the true model $Q^{b}_{0}$ is
typically unknown, more accurate estimates may be derived by employing machine
learning algorithms in the estimation procedure.

This reliance on machine learning algorithms leads naturally to the issue of
choosing an optimal data-adaptive algorithm. To address this issue, we advocate
use of the Super Learner algorithm, a generalized stacking algorithm for
ensemble learning, implemented via cross-validation, which produces an optimally
weighted combination of candidate estimators, minimizing the cross-validated
risk. Using this procedure, the predictions from a set of candidate algorithms
are combined, allowing for highly data-adaptive functional forms to be
specified~\cite{van2007super}.

Though the set of candidate algorithms in the library may be arbitrary, the
theoretical underpinnings of the Super Learner algorithm offer guidance as to
the type and number of learning algorithms that ought to be considered in the
fitting routine. In the rare case that one of the candidate learning algorithms
captures the true model and, consequently, converges to the correct estimate at
a parametric rate, the Super Learner algorithm has been shown to converge to
the same estimate at a near-parametric rate of
$O\left(\frac{log(n)}{n}\right)$~\cite{van2007super}. As true relationships are
rarely captured by single learning algorithms alone, Super Learner will, up to a
first order term, do as well (in terms of risk) as an algorithm that chooses the
particular candidate learner based on full knowledge of the true distribution
--- that is, an oracle selector --- a result that holds as long as the number of
candidate algorithms is polynomial in sample size. The principle implementation
of the Super Learner algorithm is available as a software
package~\cite{van2007super} for the R language and environment for statistical
computing~\cite{R}.

\subsection{Targeted minimum loss-based estimation}\label{tmle}

While the Super Learner estimate of $Q_0$ is performed to minimize the
cross-validated risk based on an appropriate loss-function, $Q_0$ is not the
target of our analysis, rather we seek to minimize the mean-squared error of an
estimator of $\Psi_b(P_0)$, the target parameter of interest. There is no
guarantee that, given a set of highly data-adaptive learning algorithms, the
estimate of $\Psi_b(P_0)$ will have a normal sampling distribution, even in
cases of fairly large sample size. Fortunately, an estimator of $Q_0$ that not
only ``targets'' the estimate of the regression towards the particular parameter
of interest but also ``smooths'' the estimator such that the sampling
distribution converges reliably to a normal distribution is
available~\cite{van2011targeted}. This ``targeting'' step can be thought of as
optimizing the bias-variance tradeoff, since the data-adaptive selection
procedure of Super Learner results in an estimate of $\Psi_b(P_0)$ that suffers
from residual confounding. This form of confounding can occur, for instance, if
the variable selection step in the procedure estimating $Q^b_0$ leaves out any
regressors that are, in truth, confounders of the association of $A$ and $Y$. In
this case, bias in estimation of $\Psi_b(P_0)$ is caused by under-fitting. Thus,
the resultant targeted minimum loss-based estimator (TMLE) is more robust to
model misspecification than the initial substitution estimator, based on the
initial fit of Super Learner, and is also, if one has consistent estimates of
all relevant portions of $P_0$, semiparametrically locally efficient. For a
detailed discussion of the theory of targeted minimum loss-based estimation and
formal justifications of the efficiency of the resultant estimator, consult the
appendix of van der Laan and Rose~\cite{van2011targeted}.

Algorithmically, the TMLE-based estimator in our case is a simple
one-dimensional augmentation of the initial fit. Specifically, in the case of
a continuous outcome, following the initial Super Learner fit, one proceeds by
fitting a simple, one-dimensional regression:
$$
Q^{(b, 1)}_n(A, W) = Q^{(b, 0)}_n(A, W) + \hat{\epsilon} h_{g_n}(A, W)
$$
where the initial fit, $Q^{(b, 0)}_n(A, W)$ is treated as an offset, and
$h_{g_n}(A, W)$ is a so-called ``clever'' covariate:
$$
h_{g_n}(A, W) = \frac{I(A = 1)}{g_n(1 \mid W)} - \frac{I(A = 0)}{g_n(0 \mid W)}
,$$
where $g_n(1 \mid W)$ is an estimate of $P(A = 1 \mid W)$, or the propensity
score~\cite{rosenbaum1983central}; $\hat{\epsilon}$ is the estimated coefficient
from the regression of $Y$ on $h_{g_n}(A, W)$, treating $Q^{(b, 0)}_n(A, W)$ (or
the logit of this quantity if regression is logistic) as the offset. The
selection of $g_n$ can be made via a process that minimizes the mean-squared
error of the parameter of interest~\cite{gruber2010application}, but for
application purposes, a simple main-terms logistic regression is usually
sufficient. In the final step of this procedure, the targeted minimum loss-based
estimate of $\Psi_b$ is derived using the targeted estimate of $Q$:
\begin{eqnarray}
\Psi_b(P^*_n) = \frac{1}{n}\sum^{n}_{i = 1}[Q^{(b, 1)}_n(A_i = 1, W_i) -
  Q^{(b, 1)}_{n}(A_i = 0, W_i)],
\end{eqnarray}
where $P^*_n$ is the estimate of the data-generating distribution based on TMLE,
in this case, based on estimates of $g_n, Q^{(b,1)}_n$.

\section{Statistical Inference}\label{inference}

\subsection{An influence curve-based approach}\label{ic}

As shown in~\cite{van2011targeted}, $\Psi_b(P^*_n)$ is an asymptotically
linear estimator of $\Psi_b(P_0)$, with influence curve $IC(O_i)$ if it
satisfies
\begin{eqnarray}
\sqrt{n}(\Psi_b(P^*_n) - \Psi_b(P_0)) = \frac{1}{\sqrt{n}}\sum^{n}_{i =
  1}IC(O_i) + o_p(1).
\label{eqn:IC}
\end{eqnarray}

Note from (~\ref{eqn:IC}) above that the variance of $\Psi_b(P^*_n)$ is well
approximated by the sample variance of the influence curve divided by the sample
size. When considering biomarkers, the estimated influence curve for the ATE is

\begin{dmath}
IC_{b, n}(O_i) = \left[ \frac{I(A_i = 1)}{g_n(1 \mid W_i)} - \frac{I(A_i = 0)}
  {g_n(0 \mid W_i)} \right] (Y_{b, i} - Q^{(b, 1)}_{n}(A_i, W_i)) +
  Q^{(b, 1)}_{n}(1, W_i) - Q^{(b,1)}_{n}(0, W_i) - \Psi_b(P^*_n).
\end{dmath}

With the above in hand, we easily derive asymptotic p-values and confidence
intervals (CI) with a Wald-type approach:
\begin{eqnarray}
\mbox{p-value} = 2 \left[ 1 -
  \Phi\left(\frac{\vert\Psi_b(P^*_n)\vert}{\sigma^b_n/\sqrt{n}}\right) \right]\\
\mbox{(1 - $\alpha$) CI} = \Psi_b(P^*_n) \pm
  \frac{Z_{(1 - \alpha)} \sigma^b_{n}}{\sqrt{n}}
\label{eqn:tmleInference}
\end{eqnarray}
where $\sigma^b_{n}$ is the sample standard deviation of $IC_b$ and
$\Phi(\cdot)$ is the CDF of the standard normal distribution.

\subsection{Moderated statistics for influence curve-based
estimates}\label{modtIC}

In high-dimensional settings, with small sample sizes, direct application of
TMLE for obtaining joint inference for a targeted estimate of a variable
importance measure can result in unstable standard error estimates, and thus
potentially erroneous identification of biomarkers. This is particularly
important if data-adaptive procedures are utilized, as these can add to
finite-sample non-robustness. To address this problem, we apply moderated
statistics~\cite{smyth2005limma}, a technique that preserves accurate asymptotic
inference, yet, provides robust inference in small sample settings by drawing
on information across the many estimates of sampling variability (the
$\sigma^b_n$) by invoking an empirical Bayes procedure. First developed for the
analysis of data from microarray experiments, the moderated t-statistic is
implemented in the immensely popular ``limma'' software package, which provides
a suite of tools for analyzing differential expression of genes using linear
models, borrowing information across all genes to provide stable and robust
inference for microarray data~\cite{smyth2005limma}. Previously, we noted that
a common way of making inference about the target parameter $\Psi_b(P_0)$ is to
compute the influence curve-based values for $\Psi_b(P_n^*)$, which can then be
used to calculate the corresponding standard errors of the influence curve of
the target parameter. After obtaining these IC values, finding corresponding
p-values and making inference about $\Psi_b$ for each probe follow trivially.

The procedure for using moderated statistics on IC-based estimates of $\Psi_b$,
using the approach of ``limma'' to impose variance shrinkage with an empirical
Bayes procedure, is as follows:

\begin{enumerate}
\item Assume repeated tests, across all probes $b$, of null and alternative
    hypotheses:\\$H_0: \Psi_b(P_0) = 0, H_A: \Psi_b(P_0) \ne 0 $.
\item Find influence curve-based estimates for each probe, one at a time, using
    these to iteratively build a matrix of IC-based estimates of the target
    parameter across all subjects, for all probes.
\item Since the IC-based estimates have mean zero, add in the corresponding
    estimates of $\Psi_b(P_n)$ to each row (probe). This results in each row
    having an appropriate average ($\Psi_b(P_n)$) and sample variance
    equivalent to that of the influence curve for that probe ($\text{IC}_b$).
\item Using the implementation readily available in the ``limma'' R package,
    apply the moderated t-statistic ($\tilde{t}_b, b = 1, \dots, B$) to the
    aforementioned matrix of IC-based estimates of the target parameter,
    resulting in individual estimates across each probe, relative to the null
    hypotheses above.
\item The resulting inference, based on the  shrinkage estimate of the
    sampling standard deviation of the influence curve ($\tilde{\sigma}^b_n$) is
    a weighted average of $\sigma^b_n$ and a value close to the average of all
    these sample standard deviation estimates across the biomarkers
    ($\bar{\sigma}^b_n \approx \frac{1}{B} \sum_{b = 1}^B \sigma^b_n$, or
    $\widetilde{\sigma}^b_n = wt_b \sigma^b_n+(1 - wt_b) \bar{\sigma}^b_n$,
    where $wt_b \in (0, 1)$). See~\cite{smyth2005limma} for a rigorous and
    formal presentation. Asymptotically, as $n \rightarrow \infty$,
    $wt_b \rightarrow 1$, and thus
    $\widetilde{\sigma}^b_n \rightarrow \sigma^b_n$ as desired.
\item Use a multiple testing correction procedure to obtain accurate
  simultaneous inference for all probes (biomarkers) $b = 1, \dots, B$. In
    standard practice, we recommend the well-known Benjamini-Hochberg procedure
    for controlling the False Discovery Rate
    (FDR)~\cite{benjamini1995controlling}.
\end{enumerate}

The procedure enumerated above will shrink aberrant estimates of variability
towards the center of their joint distribution, with a particularly noticeable
effect when the sample size is small. The practical effect is that application
of this procedure reduces the number of significant biomarkers, largely false
positives driven by potentially erroneous underestimates of the variation of the
estimate of the parameter of interest, $\Psi_b(P_n)$. This approach is
convenient in that it can handle any asymptotically linear estimator (has a
representation as in (~\ref{eqn:IC})), which covers many estimators of
parameters of scientific interest. An open source software package, ``biotmle,''
implementing the described procedure, is publicly
available~\cite{hejazi2017biotmle}.
